import argparse
import asyncio
import json
import sys
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Sequence, Tuple, cast

from dotenv import load_dotenv
from tqdm.auto import tqdm

from utils.external_llm_client import ExternalLLMClient
from utils.refusal_judge import RefusalJudgement, judge_prompt_response_refusal


load_dotenv()


def _load_json(path: Path) -> List[Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as fin:
        data = json.load(fin)
    if not isinstance(data, list):
        raise ValueError(f"Expected a list of prompt-response records in {path}")
    return data


def _load_records(paths: Sequence[Path]) -> Tuple[List[Dict[str, Any]], List[str]]:
    combined: List[Dict[str, Any]] = []
    sources: List[str] = []
    for path in paths:
        records = _load_json(path)
        combined.extend(records)
        sources.extend([str(path)] * len(records))
    return combined, sources


async def _evaluate_refusals_async(
    records: List[Dict[str, Any]],
    *,
    client: ExternalLLMClient,
    model_name: str,
    max_concurrency: int,
) -> Tuple[List[RefusalJudgement], int]:
    max_concurrency = max(1, max_concurrency)
    semaphore = asyncio.Semaphore(max_concurrency)
    results: List[RefusalJudgement | None] = [None] * len(records)
    refusal_count = 0

    progress = tqdm(total=len(records), desc="Judging refusals")

    async def _classify(idx: int, record: Dict[str, Any]) -> Tuple[int, RefusalJudgement]:
        async with semaphore:
            judgement = await asyncio.to_thread(
                judge_prompt_response_refusal,
                prompt=str(record.get("prompt", "")),
                response=str(record.get("response", "")),
                client=client,
                model=model_name,
            )
        return idx, judgement

    tasks = [asyncio.create_task(_classify(idx, record)) for idx, record in enumerate(records)]

    try:
        for task in asyncio.as_completed(tasks):
            idx, judgement = await task
            results[idx] = judgement
            if judgement.is_refusal:
                refusal_count += 1
            progress.update(1)
    finally:
        progress.close()

    if any(j is None for j in results):
        raise RuntimeError("Missing refusal judgements for one or more records.")

    judgements = cast(List[RefusalJudgement], results)
    return judgements, refusal_count


def _summarize_per_language(
    records: List[Dict[str, Any]],
    judgements: List[RefusalJudgement],
) -> Dict[str, Dict[str, float | int]]:
    per_language_total: Dict[str, int] = defaultdict(int)
    per_language_refusal: Dict[str, int] = defaultdict(int)

    for record, judgement in zip(records, judgements):
        language_value = record.get("language")
        if isinstance(language_value, str) and language_value:
            language_key = language_value.lower()
        else:
            language_key = "unknown"
        per_language_total[language_key] += 1
        if judgement.is_refusal:
            per_language_refusal[language_key] += 1

    summary: Dict[str, Dict[str, float | int]] = {}
    for language, total in per_language_total.items():
        refused = per_language_refusal.get(language, 0)
        summary[language] = {
            "total": total,
            "refused": refused,
            "over_refusal_rate": (refused / total) if total else 0.0,
        }

    return summary


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate over-refusals on harmless PolyRefuse prompts.")
    parser.add_argument(
        "--input-path",
        "--input-paths",
        dest="input_paths",
        type=Path,
        nargs="+",
        default=[Path("output/language-refusal/prompt_responses_harmless.json")],
        help="One or more prompt-response JSON files generated by refusal_vector_intervention for the harmless split.",
    )
    parser.add_argument(
        "--results-path",
        type=Path,
        default=Path("output/language-refusal/utility_summary.json"),
        help="Where to write aggregate utility results.",
    )
    parser.add_argument(
        "--model",
        default="gpt-5",
        help="External judge model identifier.",
    )
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=4,
        help="Maximum number of concurrent refusal judgements.",
    )

    args = parser.parse_args()

    records, record_sources = _load_records(args.input_paths)
    if not records:
        joined = ", ".join(str(path) for path in args.input_paths)
        print(f"No records found in {joined}.")
        return

    client = ExternalLLMClient(model=args.model)
    judgements, refusal_count = asyncio.run(
        _evaluate_refusals_async(
            records,
            client=client,
            model_name=args.model,
            max_concurrency=args.max_concurrency,
        )
    )

    per_language_summary = _summarize_per_language(records, judgements)

    total_examples = len(records)
    over_refusal_rate = refusal_count / total_examples if total_examples else 0.0

    per_example_results: List[Dict[str, Any]] = []
    for record, judgement, source_path in zip(records, judgements, record_sources):
        per_example_results.append(
            {
                "prompt": record.get("prompt"),
                "response": record.get("response"),
                "language": record.get("language"),
                "split": record.get("split"),
                "source": record.get("source"),
                "metadata": record.get("metadata"),
                "input_path": source_path,
                "refused": judgement.is_refusal,
                "confidence": judgement.confidence,
                "reasoning": judgement.reasoning,
            }
        )

    aggregate_results = {
        "input_files": [str(path) for path in args.input_paths],
        "total_examples": total_examples,
        "refused_examples": refusal_count,
        "over_refusal_rate": over_refusal_rate,
        "per_language": per_language_summary,
        "per_example_results": per_example_results,
    }

    args.results_path.parent.mkdir(parents=True, exist_ok=True)
    with args.results_path.open("w", encoding="utf-8") as fout:
        json.dump(aggregate_results, fout, ensure_ascii=False, indent=2)

    print(f"Evaluated {total_examples} examples.")
    print(f"Over-refusal rate: {over_refusal_rate:.3%} ({refusal_count}/{total_examples})")
    if per_language_summary:
        print("Per-language over-refusal rates:")
        for language in sorted(per_language_summary):
            stats = per_language_summary[language]
            rate = stats["over_refusal_rate"]
            refused = stats["refused"]
            total = stats["total"]
            print(f"  {language}: {rate:.3%} ({refused}/{total})")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Utility evaluation interrupted by user.")
        sys.exit(130)
